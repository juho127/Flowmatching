# ë¹„íŠ¸ì½”ì¸ ê°€ê²© ì˜ˆì¸¡: Flow Matching vs ì „í†µ ê¸°ë²• ë¹„êµ ì‹¤í—˜ ì„¤ê³„

## ğŸ“‹ 1. ì‹¤í—˜ ê°œìš”

### 1.1 ëª©ì 
- **ì£¼ ëª©í‘œ**: Flow Matching ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ì´ ë¹„íŠ¸ì½”ì¸ ê°€ê²© ì˜ˆì¸¡ì—ì„œ ì „í†µì  ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œì§€ ê²€ì¦
- **ë¶€ ëª©í‘œ**: 
  - ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ì„±ëŠ¥ ë¹„êµ
  - ê³„ì‚° íš¨ìœ¨ì„± ë¹„êµ
  - ê·¹ë‹¨ ì‹œì¥ ìƒí™©ì—ì„œì˜ robustness ë¹„êµ

### 1.2 ì—°êµ¬ ì§ˆë¬¸
1. Flow Matchingì´ ì  ì˜ˆì¸¡(point forecast) ì •í™•ë„ì—ì„œ ìš°ìˆ˜í•œê°€?
2. í™•ë¥ ì  ì˜ˆì¸¡(probabilistic forecast)ì—ì„œ ë” ë‚˜ì€ ë¶ˆí™•ì‹¤ì„± ì¶”ì •ì„ ì œê³µí•˜ëŠ”ê°€?
3. ê³„ì‚° ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œê°€?
4. ì‹œì¥ ë³€ë™ì„±ì´ í´ ë•Œ ë” robustí•œê°€?

---

## ğŸ“Š 2. ë°ì´í„°ì…‹ êµ¬ì„±

### 2.1 ë°ì´í„° ì†ŒìŠ¤
```python
# ë°ì´í„° ìˆ˜ì§‘
ì¶œì²˜: Yahoo Finance, Binance API, CoinGecko API
ê¸°ê°„: 2018-01-01 ~ 2024-12-31 (7ë…„)
ë¹ˆë„: 1ì‹œê°„ ë‹¨ìœ„ (hourly)
ì´ ìƒ˜í”Œ: ~61,000 ì‹œê°„
```

### 2.2 Feature êµ¬ì„±

#### ê¸°ë³¸ OHLCV
- `open`: ì‹œê°€
- `high`: ê³ ê°€
- `low`: ì €ê°€
- `close`: ì¢…ê°€ (ì£¼ íƒ€ê²Ÿ)
- `volume`: ê±°ë˜ëŸ‰

#### íŒŒìƒ Features (Technical Indicators)
```python
features = {
    # ê°€ê²© ê¸°ë°˜
    'returns': 'ë¡œê·¸ ìˆ˜ìµë¥ ',
    'volatility': 'ì´ë™ í‘œì¤€í¸ì°¨ (20ì‹œê°„)',
    
    # ì´ë™í‰ê· 
    'ma_7': '7ì‹œê°„ ì´ë™í‰ê· ',
    'ma_25': '25ì‹œê°„ ì´ë™í‰ê· ',
    'ma_99': '99ì‹œê°„ ì´ë™í‰ê· ',
    
    # ê¸°ìˆ  ì§€í‘œ
    'rsi': 'Relative Strength Index',
    'macd': 'MACD',
    'macd_signal': 'MACD Signal',
    'bb_upper': 'ë³¼ë¦°ì € ë°´ë“œ ìƒë‹¨',
    'bb_lower': 'ë³¼ë¦°ì € ë°´ë“œ í•˜ë‹¨',
    
    # ê±°ë˜ëŸ‰ ì§€í‘œ
    'volume_ma': 'ê±°ë˜ëŸ‰ ì´ë™í‰ê· ',
    'volume_std': 'ê±°ë˜ëŸ‰ í‘œì¤€í¸ì°¨',
}
```

#### ì™¸ë¶€ Features (ì„ íƒì )
- Fear & Greed Index
- Google Trends (Bitcoin ê²€ìƒ‰ëŸ‰)
- S&P 500 ìˆ˜ìµë¥  (ì‹œì¥ ì‹¬ë¦¬)

### 2.3 ë°ì´í„° ë¶„í• 

```python
# ì‹œê³„ì—´ íŠ¹ì„± ê³ ë ¤ (ëœë¤ ë¶„í•  X)
train_period = '2018-01-01' ~ '2022-12-31'  # 5ë…„ (87.5%)
val_period   = '2023-01-01' ~ '2023-06-30'  # 6ê°œì›” (8.75%)
test_period  = '2023-07-01' ~ '2024-12-31'  # 1.5ë…„ (3.75%)

# íŠ¹ë³„ í…ŒìŠ¤íŠ¸ì…‹
crisis_test = {
    'luna_crash': '2022-05-07' ~ '2022-05-15',     # ë£¨ë‚˜ ì‚¬íƒœ
    'ftx_collapse': '2022-11-06' ~ '2022-11-14',   # FTX íŒŒì‚°
    'high_volatility': 'volatility > 90th percentile'
}
```

### 2.4 ì •ê·œí™”
```python
# ê° featureë³„ ì •ê·œí™” (train set ê¸°ì¤€)
scaler = StandardScaler()  # ë˜ëŠ” RobustScaler

# íƒ€ê²Ÿ ì •ê·œí™”
target_scaler = StandardScaler()
# ë˜ëŠ” ë¡œê·¸ ë³€í™˜: log(price / price[t-1])
```

### 2.5 ì‹œí€€ìŠ¤ ìƒì„±
```python
window_config = {
    'lookback': 168,   # 7ì¼ (7 * 24ì‹œê°„)
    'horizon': 24,     # 1ì¼ ì˜ˆì¸¡ (24ì‹œê°„)
    'stride': 1        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
}
```

---

## ğŸ¤– 3. ë¹„êµ ëª¨ë¸ (ëŒ€ì¡°êµ°)

### 3.1 Baseline Models

#### Model 1: Naive Forecasting
```python
class NaiveForecaster:
    """
    ê°€ì¥ ë‹¨ìˆœí•œ baseline
    - Persistence: y_t+h = y_t
    - Seasonal Naive: y_t+h = y_t-s (s=24 for daily seasonality)
    """
    def predict(self, last_value):
        return last_value  # ë˜ëŠ” last_value + drift
```

#### Model 2: ARIMA / SARIMA
```python
from statsmodels.tsa.statespace.sarimax import SARIMAX

model_config = {
    'order': (5, 1, 2),              # (p, d, q)
    'seasonal_order': (1, 0, 1, 24)  # (P, D, Q, s)
}
```

### 3.2 Machine Learning Models

#### Model 3: XGBoost
```python
import xgboost as xgb

model = xgb.XGBRegressor(
    n_estimators=1000,
    max_depth=7,
    learning_rate=0.01,
    subsample=0.8,
    colsample_bytree=0.8,
    objective='reg:squarederror'
)

# Feature: ê³¼ê±° 168ì‹œê°„ flatten + technical indicators
```

#### Model 4: LightGBM
```python
import lightgbm as lgb

model = lgb.LGBMRegressor(
    n_estimators=1000,
    num_leaves=63,
    learning_rate=0.01,
    feature_fraction=0.8,
    bagging_fraction=0.8,
    bagging_freq=5
)
```

### 3.3 Deep Learning Models

#### Model 5: LSTM
```python
class LSTMForecaster(nn.Module):
    def __init__(self, input_dim, hidden_dim=128, num_layers=3):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            dropout=0.2,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_dim, 24)  # 24ì‹œê°„ ì˜ˆì¸¡
    
    def forward(self, x):
        output, (h_n, c_n) = self.lstm(x)
        return self.fc(h_n[-1])
```

#### Model 6: Transformer (Temporal Fusion Transformer)
```python
from pytorch_forecasting import TemporalFusionTransformer

model_config = {
    'hidden_size': 128,
    'attention_head_size': 4,
    'dropout': 0.1,
    'hidden_continuous_size': 16,
    'output_size': 7  # quantiles
}
```

#### Model 7: N-BEATS
```python
# Neural Basis Expansion Analysis for Time Series
class NBeats(nn.Module):
    """
    êµ¬ì¡°: Stacks of blocks
    - Trend stack
    - Seasonality stack
    - Generic stack
    """
    pass
```

### 3.4 Probabilistic Models

#### Model 8: Quantile Regression (ì—¬ëŸ¬ ë°±ë³¸ìœ¼ë¡œ)
```python
class QuantileRegression:
    """
    ë°±ë³¸: LSTM, Transformer, XGBoost
    ì†ì‹¤: Quantile Loss for [0.1, 0.25, 0.5, 0.75, 0.9]
    """
    def __init__(self, quantiles=[0.1, 0.25, 0.5, 0.75, 0.9]):
        self.quantiles = quantiles
        self.models = {q: backbone() for q in quantiles}
```

#### Model 9: Bayesian Neural Network (MC Dropout)
```python
class BayesianLSTM(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super().__init__()
        self.lstm = nn.LSTM(...)
        self.dropout = nn.Dropout(dropout_rate)
        self.fc = nn.Linear(...)
    
    def predict_with_uncertainty(self, x, n_samples=100):
        self.train()  # dropout on!
        predictions = [self.forward(x) for _ in range(n_samples)]
        return predictions
```

#### Model 10: TimeGrad (Diffusion Baseline)
```python
# ì „í†µì  ë””í“¨ì „ ëª¨ë¸ (ë¹„êµìš©)
class TimeGrad:
    """
    Diffusion ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡
    - 100-1000 ìŠ¤í…
    - Backward denoising
    """
    pass
```

### 3.5 ì œì•ˆ ëª¨ë¸

#### Model 11: Flow Matching (Main)
```python
class FlowMatchingForecaster:
    """
    ë‹¹ì‹ ì˜ ì•„ì´ë””ì–´ êµ¬í˜„
    - Forward velocity field í•™ìŠµ
    - ODE ì ë¶„ìœ¼ë¡œ ì˜ˆì¸¡
    - 1-10 ìŠ¤í…
    """
    pass
```

---

## ğŸ“ 4. í‰ê°€ ì§€í‘œ

### 4.1 ì  ì˜ˆì¸¡ ì •í™•ë„ (Point Forecast Metrics)

```python
metrics = {
    # ê¸°ë³¸ ì§€í‘œ
    'MAE': 'Mean Absolute Error',
    'RMSE': 'Root Mean Squared Error',
    'MAPE': 'Mean Absolute Percentage Error',
    
    # ê¸ˆìœµ íŠ¹í™” ì§€í‘œ
    'Directional_Accuracy': 'ë°©í–¥ ì˜ˆì¸¡ ì •í™•ë„',
    'Sharpe_Ratio': 'ì˜ˆì¸¡ ê¸°ë°˜ ì „ëµì˜ ìƒ¤í”„ ë¹„ìœ¨',
    
    # ì‹œê°„ëŒ€ë³„
    'MAE_1h': '1ì‹œê°„ í›„ MAE',
    'MAE_6h': '6ì‹œê°„ í›„ MAE',
    'MAE_24h': '24ì‹œê°„ í›„ MAE',
}
```

### 4.2 í™•ë¥ ì  ì˜ˆì¸¡ í’ˆì§ˆ (Probabilistic Forecast Metrics)

```python
probabilistic_metrics = {
    # Calibration
    'CRPS': 'Continuous Ranked Probability Score',
    'Pinball_Loss': 'ê° quantileì˜ í‰ê·  ì†ì‹¤',
    'Coverage': 'ì‹¤ì œê°’ì´ ì˜ˆì¸¡ êµ¬ê°„ ë‚´ì— ìˆëŠ” ë¹„ìœ¨',
    
    # Sharpness
    'Interval_Width': 'ì˜ˆì¸¡ êµ¬ê°„ì˜ í‰ê·  í­',
    'Std_Prediction': 'ì˜ˆì¸¡ ë¶„í¬ì˜ í‘œì¤€í¸ì°¨',
    
    # ì¢…í•©
    'Winkler_Score': 'coverage + sharpness ê²°í•©',
}
```

### 4.3 ê³„ì‚° íš¨ìœ¨ì„±

```python
efficiency_metrics = {
    'Training_Time': 'í•™ìŠµ ì‹œê°„ (ì´ˆ)',
    'Inference_Time': '1000 ìƒ˜í”Œ ì˜ˆì¸¡ ì‹œê°„ (ì´ˆ)',
    'Memory_Usage': 'ìµœëŒ€ GPU/CPU ë©”ëª¨ë¦¬ (GB)',
    'Parameters': 'ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜',
    'FLOPs': 'ì—°ì‚°ëŸ‰',
}
```

### 4.4 Robustness ì§€í‘œ

```python
robustness = {
    # ê·¹ë‹¨ ìƒí™© ì„±ëŠ¥
    'High_Vol_MAE': 'ë³€ë™ì„± ìƒìœ„ 10% êµ¬ê°„ MAE',
    'Crisis_MAE': 'ìœ„ê¸° êµ¬ê°„ (Luna, FTX) MAE',
    
    # ì•ˆì •ì„±
    'Prediction_Variance': 'ê°™ì€ ì…ë ¥ì— ëŒ€í•œ ì˜ˆì¸¡ ë¶„ì‚°',
    'Outlier_Sensitivity': 'ì´ìƒì¹˜ì— ëŒ€í•œ ë¯¼ê°ë„',
}
```

---

## âš™ï¸ 5. ì‹¤í—˜ ì„¤ì •

### 5.1 í•˜ë“œì›¨ì–´

```yaml
environment:
  GPU: NVIDIA RTX 4090 24GB (or A100)
  CPU: AMD Ryzen 9 / Intel i9
  RAM: 64GB
  Storage: 1TB SSD
```

### 5.2 ì†Œí”„íŠ¸ì›¨ì–´ ìŠ¤íƒ

```python
requirements = {
    'python': '3.10+',
    'pytorch': '2.1.0',
    'numpy': '1.24.0',
    'pandas': '2.0.0',
    'scikit-learn': '1.3.0',
    
    # ë°ì´í„°
    'yfinance': '0.2.28',
    'ccxt': '4.0.0',  # ì•”í˜¸í™”í ê±°ë˜ì†Œ API
    'ta': '0.11.0',   # Technical Analysis
    
    # ì‹œê°í™”
    'matplotlib': '3.7.0',
    'seaborn': '0.12.0',
    'plotly': '5.17.0',
    
    # ì‹¤í—˜ ê´€ë¦¬
    'wandb': '0.15.0',  # ë˜ëŠ” mlflow
    'hydra-core': '1.3.0',  # config ê´€ë¦¬
}
```

### 5.3 í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰

```python
# Optuna ì‚¬ìš©
import optuna

search_space = {
    # ê³µí†µ
    'learning_rate': [1e-5, 1e-2],  # log scale
    'batch_size': [32, 64, 128, 256],
    'hidden_dim': [64, 128, 256, 512],
    
    # Flow Matching ì „ìš©
    'num_flow_steps': [1, 5, 10, 20, 50],
    'velocity_net_layers': [2, 3, 4, 5],
    
    # ì •ê·œí™”
    'dropout': [0.1, 0.5],
    'weight_decay': [1e-6, 1e-3],
}

# ê° ëª¨ë¸ë‹¹ 100 trials
```

### 5.4 í›ˆë ¨ ì„¤ì •

```python
training_config = {
    'epochs': 100,
    'early_stopping_patience': 15,
    'lr_scheduler': 'CosineAnnealingWarmRestarts',
    'optimizer': 'AdamW',
    'grad_clip': 1.0,
    
    # ë°°ì¹˜
    'batch_size': 128,
    'num_workers': 8,
    
    # ê²€ì¦
    'val_check_interval': 500,  # steps
    'save_top_k': 3,
}
```

---

## ğŸ—ï¸ 6. êµ¬í˜„ ì•„í‚¤í…ì²˜

### 6.1 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
bitcoin-forecasting/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # ì›ë³¸ ë°ì´í„°
â”‚   â”œâ”€â”€ processed/              # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â””â”€â”€ features/               # Feature engineering ê²°ê³¼
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ data_loader.py      # ë°ì´í„° ë¡œë”©
â”‚   â”‚   â”œâ”€â”€ preprocessor.py     # ì „ì²˜ë¦¬
â”‚   â”‚   â””â”€â”€ feature_engineering.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ baselines/
â”‚   â”‚   â”‚   â”œâ”€â”€ naive.py
â”‚   â”‚   â”‚   â”œâ”€â”€ arima.py
â”‚   â”‚   â”‚   â””â”€â”€ ml_models.py    # XGBoost, LightGBM
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ deep_learning/
â”‚   â”‚   â”‚   â”œâ”€â”€ lstm.py
â”‚   â”‚   â”‚   â”œâ”€â”€ transformer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ nbeats.py
â”‚   â”‚   â”‚   â””â”€â”€ quantile_reg.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ probabilistic/
â”‚   â”‚   â”‚   â”œâ”€â”€ bayesian_nn.py
â”‚   â”‚   â”‚   â””â”€â”€ timegrad.py     # Diffusion baseline
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ flow_matching/
â”‚   â”‚       â”œâ”€â”€ flow_net.py     # ì œì•ˆ ëª¨ë¸
â”‚   â”‚       â”œâ”€â”€ velocity_field.py
â”‚   â”‚       â””â”€â”€ ode_solver.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py          # í†µí•© í•™ìŠµ ë£¨í”„
â”‚   â”‚   â”œâ”€â”€ losses.py           # ì†ì‹¤ í•¨ìˆ˜ë“¤
â”‚   â”‚   â””â”€â”€ callbacks.py        # ì½œë°±
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ metrics.py          # í‰ê°€ ì§€í‘œ
â”‚   â”‚   â”œâ”€â”€ evaluator.py        # í‰ê°€ ì‹¤í–‰
â”‚   â”‚   â””â”€â”€ statistical_tests.py # í†µê³„ ê²€ì •
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py           # ì„¤ì • ê´€ë¦¬
â”‚       â”œâ”€â”€ logger.py           # ë¡œê¹…
â”‚       â””â”€â”€ visualization.py    # ì‹œê°í™”
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ configs/                # Hydra configs
â”‚   â”œâ”€â”€ notebooks/              # ë¶„ì„ ë…¸íŠ¸ë¶
â”‚   â””â”€â”€ scripts/                # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚
â”œâ”€â”€ tests/                      # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ results/                    # ì‹¤í—˜ ê²°ê³¼
â””â”€â”€ docs/                       # ë¬¸ì„œ
```

### 6.2 í•µì‹¬ ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤

```python
# models/base.py
class BaseForecaster(ABC):
    @abstractmethod
    def fit(self, train_data, val_data):
        """ëª¨ë¸ í•™ìŠµ"""
        pass
    
    @abstractmethod
    def predict(self, x, num_samples=1):
        """ì˜ˆì¸¡ (í™•ë¥ ì  ìƒ˜í”Œë§ ì§€ì›)"""
        pass
    
    def evaluate(self, test_data, metrics):
        """í‰ê°€"""
        pass

# models/flow_matching/flow_net.py
class FlowMatchingForecaster(BaseForecaster):
    def __init__(self, config):
        self.model = TimeSeriesFlowNet(config)
        self.forecaster = FlowForecaster(self.model)
    
    def fit(self, train_data, val_data):
        # í•™ìŠµ ë¡œì§
        pass
    
    def predict(self, x, num_samples=10, num_steps=10):
        # Flow ODE ì ë¶„
        return self.forecaster.predict(x, num_steps, num_samples)
```

---

## ğŸ“ˆ 7. ì‹¤í—˜ í”„ë¡œí† ì½œ

### 7.1 ì‹¤í—˜ ë‹¨ê³„

#### Phase 1: ë°ì´í„° ì¤€ë¹„ ë° EDA
```python
tasks = [
    '1. ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬',
    '2. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„',
    '3. Feature engineering',
    '4. ë°ì´í„° ë¶„í•  ë° ê²€ì¦',
]
```

#### Phase 2: Baseline êµ¬ì¶•
```python
tasks = [
    '1. Naive ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€',
    '2. ARIMA ëª¨ë¸ íŠœë‹',
    '3. ML ëª¨ë¸ (XGBoost, LightGBM) êµ¬í˜„',
    '4. Baseline ì„±ëŠ¥ í™•ë¦½',
]
```

#### Phase 3: Deep Learning ëª¨ë¸
```python
tasks = [
    '1. LSTM êµ¬í˜„ ë° íŠœë‹',
    '2. Transformer êµ¬í˜„',
    '3. N-BEATS êµ¬í˜„',
    '4. ì„±ëŠ¥ ë¹„êµ',
]
```

#### Phase 4: Probabilistic ëª¨ë¸
```python
tasks = [
    '1. Quantile Regression êµ¬í˜„',
    '2. Bayesian NN êµ¬í˜„',
    '3. TimeGrad (Diffusion baseline) êµ¬í˜„',
    '4. ë¶ˆí™•ì‹¤ì„± í‰ê°€',
]
```

#### Phase 5: Flow Matching (Main)
```python
tasks = [
    '1. Flow Matching êµ¬í˜„',
    '2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹',
    '3. Ablation study',
    '4. ì¢…í•© í‰ê°€',
]
```

#### Phase 6: ë¶„ì„ ë° ë¬¸ì„œí™”
```python
tasks = [
    '1. í†µê³„ì  ìœ ì˜ì„± ê²€ì •',
    '2. ì‹œê°í™” ë° í•´ì„',
    '3. ë…¼ë¬¸/ë¦¬í¬íŠ¸ ì‘ì„±',
    '4. ì½”ë“œ ì •ë¦¬ ë° ë¬¸ì„œí™”',
]
```

### 7.2 Cross-Validation ì „ëµ

```python
# Walk-Forward Validation
num_folds = 5
test_size = 365 * 24  # 1ë…„ (ì‹œê°„ ë‹¨ìœ„)

for i in range(num_folds):
    train_end = initial_train_end + i * test_size
    test_start = train_end
    test_end = test_start + test_size
    
    # í•™ìŠµ ë° í‰ê°€
    model.fit(data[:train_end])
    predictions = model.predict(data[test_start:test_end])
    evaluate(predictions)
```

### 7.3 í†µê³„ì  ê²€ì •

```python
statistical_tests = {
    # ëª¨ë¸ ê°„ ë¹„êµ
    'Diebold_Mariano': 'ì˜ˆì¸¡ ì •í™•ë„ ì°¨ì´ ê²€ì •',
    'Wilcoxon_Signed_Rank': 'ë¹„ëª¨ìˆ˜ ë¹„êµ',
    
    # Calibration
    'Kolmogorov_Smirnov': 'ë¶„í¬ ì í•©ë„ ê²€ì •',
    'Chi_Square': 'ë²”ì£¼ë³„ coverage ê²€ì •',
}

# ìœ ì˜ìˆ˜ì¤€
alpha = 0.05

# Bonferroni ë³´ì • (ë‹¤ì¤‘ ë¹„êµ)
adjusted_alpha = alpha / num_comparisons
```

---

## ğŸ“Š 8. ì˜ˆìƒ ê²°ê³¼ ë° ê°€ì„¤

### 8.1 ì£¼ìš” ê°€ì„¤

**H1: ì  ì˜ˆì¸¡ ì •í™•ë„**
- Flow Matchingì´ LSTM, Transformerë³´ë‹¤ MAE/RMSEê°€ ë‚®ì„ ê²ƒ
- íŠ¹íˆ ì¥ê¸° ì˜ˆì¸¡(12-24ì‹œê°„)ì—ì„œ ìš°ìœ„

**H2: ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”**
- Flow Matchingì´ TimeGradë³´ë‹¤ CRPSê°€ ë‚®ê³  calibrationì´ ì¢‹ì„ ê²ƒ
- Coverageê°€ ëª…ëª© ìˆ˜ì¤€(ì˜ˆ: 90%)ì— ê°€ê¹Œìš¸ ê²ƒ

**H3: ê³„ì‚° íš¨ìœ¨ì„±**
- Flow Matchingì´ TimeGradë³´ë‹¤ 10-100ë°° ë¹ ë¥¼ ê²ƒ
- ì¶”ë¡  ì‹œê°„ì´ ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”©ì— ì í•©í•  ê²ƒ (< 1ì´ˆ)

**H4: Robustness**
- ê³ ë³€ë™ì„± êµ¬ê°„ì—ì„œ Flow Matchingì´ ë” ì•ˆì •ì 
- ìœ„ê¸° ìƒí™©ì—ì„œë„ í•©ë¦¬ì ì¸ ì˜ˆì¸¡

### 8.2 ì˜ˆìƒ ê²°ê³¼ í‘œ

| ëª¨ë¸ | MAE (24h) | CRPS | ì¶”ë¡  ì‹œê°„ | Coverage (90%) |
|------|-----------|------|-----------|----------------|
| Naive | 2500 | - | 0.001s | - |
| ARIMA | 2200 | - | 0.1s | - |
| XGBoost | 1800 | - | 0.01s | - |
| LSTM | 1500 | 120 | 0.05s | 85% |
| Transformer | 1400 | 110 | 0.1s | 87% |
| Quantile Reg | 1450 | 105 | 0.05s | 89% |
| Bayesian NN | 1480 | 115 | 5s | 88% |
| TimeGrad | **1300** | 95 | 50s | 91% |
| **Flow Matching** | **1280** | **90** | **0.5s** | **90%** |

*ì£¼: ê°€ìƒ ìˆ˜ì¹˜, ì‹¤í—˜ í›„ ì—…ë°ì´íŠ¸*

---

## ğŸ”¬ 9. Ablation Study

### 9.1 Flow Matching ë³€í˜•

```python
ablation_experiments = {
    # 1. ODE ìŠ¤í… ìˆ˜
    'flow_steps': [1, 5, 10, 20, 50],
    
    # 2. ì¡°ê±´ ì¸ì½”ë”
    'condition_encoder': ['LSTM', 'Transformer', 'CNN'],
    
    # 3. Velocity Network êµ¬ì¡°
    'velocity_net': ['MLP', 'ResNet', 'Transformer'],
    
    # 4. ì´ˆê¸°í™” ë°©ì‹
    'initialization': [
        'last_value',      # ë§ˆì§€ë§‰ ê°’
        'gaussian',        # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ
        'learned'          # í•™ìŠµ ê°€ëŠ¥í•œ ì´ˆê¸°í™”
    ],
    
    # 5. ì‹œê°„ ì„ë² ë”©
    'time_embedding': ['sinusoidal', 'learned', 'none'],
}
```

### 9.2 Feature Ablation

```python
feature_groups = {
    'baseline': ['close'],
    '+ technical': ['close', 'rsi', 'macd', 'bb'],
    '+ volume': ['close', 'rsi', 'macd', 'bb', 'volume'],
    '+ external': ['close', ..., 'fear_greed', 'trends'],
}

# ê° ì¡°í•©ìœ¼ë¡œ í•™ìŠµ ë° í‰ê°€
```

---

## ğŸ“ 10. ë¬¸ì„œí™” ë° ì¬í˜„ì„±

### 10.1 ì‹¤í—˜ ì¶”ì 

```python
# WandB ì„¤ì •
import wandb

wandb.init(
    project="bitcoin-flow-matching",
    config={
        "model": "flow_matching",
        "dataset": "bitcoin_hourly",
        "lookback": 168,
        "horizon": 24,
    }
)

# ë©”íŠ¸ë¦­ ë¡œê¹…
wandb.log({
    "train_loss": loss,
    "val_mae": mae,
    "test_crps": crps,
})

# ëª¨ë¸ ì €ì¥
wandb.save("model.pth")
```

### 10.2 ì¬í˜„ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
reproducibility = {
    'âœ“ Random seed ê³ ì •': 'torch.manual_seed(42)',
    'âœ“ ë°ì´í„° ë²„ì „ ê´€ë¦¬': 'DVC ë˜ëŠ” Git LFS',
    'âœ“ í™˜ê²½ ê³ ì •': 'requirements.txt, Dockerfile',
    'âœ“ ì½”ë“œ ë²„ì „': 'Git commit hash ê¸°ë¡',
    'âœ“ ì„¤ì • íŒŒì¼ ì €ì¥': 'Hydra config ìë™ ì €ì¥',
    'âœ“ í•˜ë“œì›¨ì–´ ì •ë³´': 'GPU ëª¨ë¸, CUDA ë²„ì „',
}
```

### 10.3 ê²°ê³¼ ë³´ê³ ì„œ êµ¬ì¡°

```markdown
# ìµœì¢… ë³´ê³ ì„œ ëª©ì°¨

1. Executive Summary
2. Introduction & Motivation
3. Related Work
4. Methodology
   4.1 Data
   4.2 Models
   4.3 Evaluation
5. Results
   5.1 Main Results
   5.2 Ablation Study
   5.3 Statistical Tests
6. Discussion
   6.1 Interpretation
   6.2 Limitations
   6.3 Future Work
7. Conclusion
8. Appendix
   8.1 Hyperparameters
   8.2 Additional Plots
   8.3 Code Repository
```

---

## ğŸ¯ 11. ë§ˆì¼ìŠ¤í†¤ ë° íƒ€ì„ë¼ì¸

### 11.1 ì˜ˆìƒ ì¼ì • (3ê°œì›”)

```gantt
Week 1-2:   ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
Week 3-4:   Baseline ëª¨ë¸ êµ¬í˜„
Week 5-6:   Deep Learning ëª¨ë¸ êµ¬í˜„
Week 7-8:   Probabilistic ëª¨ë¸ êµ¬í˜„
Week 9-10:  Flow Matching êµ¬í˜„ ë° íŠœë‹
Week 11:    Ablation study ë° ë¶„ì„
Week 12:    ë…¼ë¬¸/ë³´ê³ ì„œ ì‘ì„±
```

### 11.2 ì²´í¬í¬ì¸íŠ¸

```python
checkpoints = {
    'Week 2': 'ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ì„±',
    'Week 4': 'Baseline ì„±ëŠ¥ í™•ë¦½',
    'Week 6': 'DL ëª¨ë¸ êµ¬í˜„ ì™„ë£Œ',
    'Week 8': 'Probabilistic ëª¨ë¸ ì™„ë£Œ',
    'Week 10': 'Flow Matching ìµœì í™” ì™„ë£Œ',
    'Week 11': 'í†µê³„ ë¶„ì„ ì™„ë£Œ',
    'Week 12': 'ìµœì¢… ë³´ê³ ì„œ ì œì¶œ',
}
```

---

## ğŸ“š 12. ì°¸ê³  ë¬¸í—Œ

### 12.1 í•µì‹¬ ë…¼ë¬¸

```bibtex
@article{lipman2022flow,
  title={Flow Matching for Generative Modeling},
  author={Lipman, Yaron and others},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@article{rasul2021autoregressive,
  title={Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting},
  author={Rasul, Kashif and others},
  journal={ICML},
  year={2021}
}

@article{liu2023rectified,
  title={Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
  author={Liu, Xingchao and others},
  journal={ICLR},
  year={2023}
}
```

### 12.2 ê´€ë ¨ ìë£Œ

- PyTorch Forecasting Documentation
- Temporal Fusion Transformer Paper
- Bitcoin Price Prediction: A Survey
- Financial Time Series Forecasting: Best Practices

---

## ğŸ’¡ 13. ì¶”ê°€ ê³ ë ¤ì‚¬í•­

### 13.1 ìœ¤ë¦¬ ë° ì±…ì„

```python
ethical_considerations = {
    'íˆ¬ì ê¶Œìœ  ì•„ë‹˜': 'í•™ìˆ  ì—°êµ¬ ëª©ì ì„ì„ ëª…ì‹œ',
    'ë¦¬ìŠ¤í¬ ê³ ì§€': 'ëª¨ë¸ì˜ í•œê³„ ëª…í™•íˆ ì„¤ëª…',
    'ì¬í˜„ì„±': 'ì½”ë“œ ë° ë°ì´í„° ê³µê°œ',
    'íˆ¬ëª…ì„±': 'ì‹¤íŒ¨í•œ ì‹¤í—˜ë„ ë³´ê³ ',
}
```

### 13.2 í™•ì¥ ê°€ëŠ¥ì„±

```python
future_extensions = {
    'ë‹¤ì¤‘ ìì‚°': 'ë¹„íŠ¸ì½”ì¸ ì™¸ ì•ŒíŠ¸ì½”ì¸ ì¶”ê°€',
    'ë©€í‹°ëª¨ë‹¬': 'ë‰´ìŠ¤, ì†Œì…œë¯¸ë””ì–´ ë°ì´í„° í†µí•©',
    'ê°•í™”í•™ìŠµ': 'íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸ ê°œë°œ',
    'ì‹¤ì‹œê°„': 'ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬',
}
```

---

## âœ… 14. ì²´í¬ë¦¬ìŠ¤íŠ¸

### 14.1 êµ¬í˜„ ì „

- [ ] ë°ì´í„° ìˆ˜ì§‘ API ì ‘ê·¼ í™•ì¸
- [ ] GPU í™˜ê²½ ì„¤ì •
- [ ] Git repository ìƒì„±
- [ ] WandB í”„ë¡œì íŠ¸ ìƒì„±
- [ ] ë¬¸í—Œ ì¡°ì‚¬ ì™„ë£Œ

### 14.2 êµ¬í˜„ ì¤‘

- [ ] ë°ì´í„° íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
- [ ] ê° ëª¨ë¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- [ ] í•™ìŠµ ì•ˆì •ì„± í™•ì¸
- [ ] ì¤‘ê°„ ê²°ê³¼ ì €ì¥
- [ ] ì½”ë“œ ë¦¬ë·°

### 14.3 êµ¬í˜„ í›„

- [ ] ëª¨ë“  í‰ê°€ ì§€í‘œ ê³„ì‚°
- [ ] í†µê³„ ê²€ì • ìˆ˜í–‰
- [ ] ì‹œê°í™” ìƒì„±
- [ ] ë¬¸ì„œ ì‘ì„±
- [ ] ì½”ë“œ ì •ë¦¬ ë° ì£¼ì„
- [ ] README ì—…ë°ì´íŠ¸
- [ ] ê²°ê³¼ ë°œí‘œ ì¤€ë¹„

---

## ğŸ“§ ì—°ë½ì²˜ ë° í˜‘ì—…

```
í”„ë¡œì íŠ¸ ê´€ë¦¬ì: [ì´ë¦„]
Email: [ì´ë©”ì¼]
GitHub: [ë ˆí¬ì§€í† ë¦¬ ë§í¬]
WandB: [í”„ë¡œì íŠ¸ ë§í¬]
```

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-10-05
**ë²„ì „**: 1.0
**ìƒíƒœ**: ì„¤ê³„ ì™„ë£Œ, êµ¬í˜„ ëŒ€ê¸°